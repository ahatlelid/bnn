{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# parameters for analytical solution and loss function\n",
    "d = {} # d is a dictonary holding all values needed for training\n",
    "tensor_D = torch.tensor(\n",
    "    [[1, -1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, -1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0,  1, -1, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0,  0, 1, -1, 0, 0, 0, 0, 0],\n",
    "    [0, 0,  0, 0, 1, -1, 0, 0, 0, 0],\n",
    "    [0, 0,  0, 0, 0, 1, -1, 0, 0, 0],\n",
    "    [0, 0,  0, 0, 0, 0, 1, -1, 0, 0],\n",
    "    [0, 0,  0, 0, 0, 0, 0, 1, -1, 0],\n",
    "    [0, 0,  0, 0, 0, 0, 0, 0, 1, -1],\n",
    "    [-1, 0, 0, 0, 0, 0, 0, 0, 0, 1],], dtype=torch.float\n",
    ")\n",
    "tensor_Q_m  = torch.mm(torch.t(tensor_D), tensor_D)\n",
    "n_param = tensor_D.size(dim=0)\n",
    "tau2 = 1000 # 1/tau2 is the noise added\n",
    "tensor_Q_m_modified = tensor_Q_m + torch.eye(n_param)*(1./tau2)\n",
    "sigma2_eps = 0.01\n",
    "tensor_mu_m = torch.zeros(n_param)\n",
    "\n",
    "d['tensor_D'] = tensor_D\n",
    "d['tensor_Q_m'] = tensor_Q_m \n",
    "d['n_param'] = n_param\n",
    "d['tau2'] = 1000  \n",
    "d['tensor_Q_m_modified'] = tensor_Q_m_modified\n",
    "d['tensor_Sigma_m'] = torch.inverse(tensor_Q_m_modified)\n",
    "d['sigma2_eps'] = sigma2_eps\n",
    "d['tensor_Sigma_eps'] = torch.eye(n_param)*sigma2_eps\n",
    "d['tensor_mu_m'] = tensor_mu_m\n",
    "d['tensor_mu_eps'] = tensor_mu_m \n",
    "\n",
    "# data options\n",
    "d['generate_data'] = False\n",
    "d['generate_data_noise'] = False\n",
    "d['generate_parameter_noise'] = False\n",
    "d['generate_initial_weight_values'] = False\n",
    "\n",
    "# path to data folders\n",
    "d['path_to_data_folder'] = '../data/...'\n",
    "d['path_to_data_noise_folder'] = '../data/..'\n",
    "d['path_to_parameter_noise_folder'] = '../data/..'\n",
    "d['path_to_weight_initial_values'] = '../data/..'\n",
    "\n",
    "# output folder\n",
    "d['path_to_output_model'] = '../saved_models/..'\n",
    "\n",
    "# runs\n",
    "d['number_of_runs'] = 100\n",
    "\n",
    "# data options\n",
    "number_of_data = 100000\n",
    "d['number_of_data'] = number_of_data\n",
    "d['batch_size'] = 1000\n",
    "d['n_batches'] = 100 # n_batches*batch_size=number_of_data\n",
    "d['n_epochs'] = 100\n",
    "\n",
    "# scaling factors\n",
    "prior_variance = 0.5**2\n",
    "d['prior_variance'] = prior_variance\n",
    "d['data_fit_scale_factor'] = 0.01\n",
    "d['regularization_scale_factor'] = 1\n",
    "d['prior_scale_factor'] = 1./(number_of_data*prior_variance)\n",
    "\n",
    "# randomized maximum likelihood\n",
    "d['sign_of_noise'] = 'positive'\n",
    "\n",
    "# initial weight\n",
    "d['initial_weight'] = 'default'\n",
    "\n",
    "# learning rate options\n",
    "d['initial_lr'] = 0.1\n",
    "d['lowest_lr'] = 1e-5\n",
    "d['reduction_factor'] = 0.2\n",
    "d['lr_automatic_ajust'] = False # True if lr should drop depenidng on loss, False if it should drop depending on epoch\n",
    "d['epochs_with_same_lr'] = 20\n",
    "d['span'] = 20\n",
    "d['delay'] = 20 \n",
    "d['display_info'] = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "d"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'tensor_D': tensor([[ 1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1., -1.],\n",
       "         [-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]]),\n",
       " 'tensor_Q_m': tensor([[ 2., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
       "         [-1.,  2., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0., -1.,  2., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0., -1.,  2., -1.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., -1.,  2., -1.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0., -1.,  2., -1.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0., -1.,  2., -1.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0., -1.,  2., -1.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  2., -1.],\n",
       "         [-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  2.]]),\n",
       " 'n_param': 10,\n",
       " 'tau2': 1000,\n",
       " 'tensor_Q_m_modified': tensor([[ 2.0010, -1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000, -1.0000],\n",
       "         [-1.0000,  2.0010, -1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000],\n",
       "         [ 0.0000, -1.0000,  2.0010, -1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000, -1.0000,  2.0010, -1.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000, -1.0000,  2.0010, -1.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000, -1.0000,  2.0010, -1.0000,  0.0000,\n",
       "           0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.0000,  2.0010, -1.0000,\n",
       "           0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.0000,  2.0010,\n",
       "          -1.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.0000,\n",
       "           2.0010, -1.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          -1.0000,  2.0010]]),\n",
       " 'tensor_Sigma_m': tensor([[100.8307, 100.3811, 100.0319,  99.7827,  99.6333,  99.5835,  99.6333,\n",
       "           99.7827, 100.0319, 100.3811],\n",
       "         [100.3811, 100.8307, 100.3811, 100.0319,  99.7827,  99.6333,  99.5835,\n",
       "           99.6333,  99.7827, 100.0319],\n",
       "         [100.0319, 100.3811, 100.8307, 100.3811, 100.0319,  99.7827,  99.6333,\n",
       "           99.5835,  99.6333,  99.7827],\n",
       "         [ 99.7827, 100.0319, 100.3811, 100.8307, 100.3811, 100.0319,  99.7827,\n",
       "           99.6333,  99.5835,  99.6333],\n",
       "         [ 99.6333,  99.7827, 100.0319, 100.3811, 100.8307, 100.3811, 100.0319,\n",
       "           99.7827,  99.6333,  99.5835],\n",
       "         [ 99.5835,  99.6333,  99.7827, 100.0319, 100.3811, 100.8307, 100.3811,\n",
       "          100.0319,  99.7827,  99.6333],\n",
       "         [ 99.6333,  99.5835,  99.6333,  99.7827, 100.0319, 100.3811, 100.8307,\n",
       "          100.3811, 100.0319,  99.7827],\n",
       "         [ 99.7827,  99.6333,  99.5835,  99.6333,  99.7827, 100.0319, 100.3811,\n",
       "          100.8307, 100.3811, 100.0319],\n",
       "         [100.0319,  99.7827,  99.6333,  99.5835,  99.6333,  99.7827, 100.0319,\n",
       "          100.3811, 100.8307, 100.3811],\n",
       "         [100.3811, 100.0319,  99.7827,  99.6333,  99.5835,  99.6333,  99.7827,\n",
       "          100.0319, 100.3811, 100.8307]]),\n",
       " 'sigma2_eps': 0.01,\n",
       " 'tensor_Sigma_eps': tensor([[0.0100, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0100, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0100, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0100, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0100, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0100, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0100, 0.0000, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0100, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0100,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0100]]),\n",
       " 'tensor_mu_m': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'tensor_mu_eps': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'generate_data': False,\n",
       " 'generate_data_noise': False,\n",
       " 'generate_parameter_noise': False,\n",
       " 'generate_initial_weight_values': False,\n",
       " 'path_to_data_folder': '../data/...',\n",
       " 'path_to_data_noise_folder': '../data/..',\n",
       " 'path_to_parameter_noise_folder': '../data/..',\n",
       " 'path_to_weight_initial_values': '../data/..',\n",
       " 'path_to_output_model': '../saved_models/..',\n",
       " 'number_of_runs': 100,\n",
       " 'number_of_data': 100000,\n",
       " 'batch_size': 1000,\n",
       " 'n_batches': 100,\n",
       " 'n_epochs': 100,\n",
       " 'prior_variance': 0.25,\n",
       " 'data_fit_scale_factor': 0.01,\n",
       " 'regularization_scale_factor': 1,\n",
       " 'prior_scale_factor': 4e-05,\n",
       " 'sign_of_noise': 'positive',\n",
       " 'initial_weight': 'default',\n",
       " 'initial_lr': 0.1,\n",
       " 'lowest_lr': 1e-05,\n",
       " 'reduction_factor': 0.2,\n",
       " 'lr_automatic_ajust': False,\n",
       " 'epochs_with_same_lr': 20,\n",
       " 'span': 20,\n",
       " 'delay': 20,\n",
       " 'display_info': True}"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "torch.save(d, 'dict.pt')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "c = torch.load('dict.pt')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "locals().update(c)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "tensor_Sigma_eps"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.0100, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0100, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0100, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0100, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0100, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0100, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0100, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0100, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0100,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0100]])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "c['tensor_D']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1., -1.],\n",
       "        [-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import json\n",
    "with open('info_dictionary.txt', 'w') as outfile:\n",
    "     outfile.write(json.dumps(d))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Object of type Tensor is not JSON serializable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo_dictionary.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outfile:\n\u001b[0;32m----> 3\u001b[0m      outfile\u001b[38;5;241m.\u001b[39mwrite(\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch_env/lib/python3.10/json/__init__.py:231\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# cached encoder\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m skipkeys \u001b[38;5;129;01mand\u001b[39;00m ensure_ascii \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     check_circular \u001b[38;5;129;01mand\u001b[39;00m allow_nan \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m separators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sort_keys \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch_env/lib/python3.10/json/encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch_env/lib/python3.10/json/encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[1;32m    254\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch_env/lib/python3.10/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type Tensor is not JSON serializable"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('info_dictionary.txt') as infile:\n",
    "    info_dictionary = infile.read()\n",
    "      \n",
    "d = json.loads(info_dictionary)\n",
    "  \n",
    "print(\"Data type after reconstruction : \", type(d))\n",
    "print(d)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data type after reconstruction :  <class 'dict'>\n",
      "{'first': 1, 'second': 2}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "g = {}\n",
    "#e['tens'] = torch.tensor([1])\n",
    "g['tens'] = np.array([1])\n",
    "\n",
    "with open('out.txt', 'w') as outfile:\n",
    "     outfile.write(json.dumps(g))\n",
    "\n",
    "with open('out.txt') as infile:\n",
    "    info_dictionary = infile.read()\n",
    "      \n",
    "h = json.loads(info_dictionary)\n",
    "tens = h['tens']  \n",
    "print(h)\n",
    "print(tens)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Object of type ndarray is not JSON serializable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m g[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outfile:\n\u001b[0;32m----> 9\u001b[0m      outfile\u001b[38;5;241m.\u001b[39mwrite(\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout.txt\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m infile:\n\u001b[1;32m     12\u001b[0m     info_dictionary \u001b[38;5;241m=\u001b[39m infile\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch_env/lib/python3.10/json/__init__.py:231\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# cached encoder\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m skipkeys \u001b[38;5;129;01mand\u001b[39;00m ensure_ascii \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     check_circular \u001b[38;5;129;01mand\u001b[39;00m allow_nan \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m separators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sort_keys \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch_env/lib/python3.10/json/encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch_env/lib/python3.10/json/encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[1;32m    254\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch_env/lib/python3.10/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type ndarray is not JSON serializable"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.10.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.4 64-bit ('pytorch_env': conda)"
  },
  "interpreter": {
   "hash": "24726422bde522ec9d6b8250d1636c6a7c473f035d7185ba3e11c63b1681e397"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}